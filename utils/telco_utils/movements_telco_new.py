import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
import json
import numpy as np
from countryinfo import CountryInfo
from collections import defaultdict
from tqdm import tqdm
import os

def run(country):
    class defaultlist(list):
        def __init__(self, fx):
            self._fx = fx
        def _fill(self, index):
            while len(self) <= index:
                self.append(self._fx())
        def __setitem__(self, index, value):
            self._fill(index)
            list.__setitem__(self, index, value)
        def __getitem__(self, index):
            self._fill(index)
            return list.__getitem__(self, index)

    def convert(o):
        if isinstance(o, np.int64): 
            return int(o)
        else:
            return o

    def percent_change(crisis, baseline):
        if baseline == 0:
            return 0
        return (crisis - baseline) / baseline

    class NpEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return super(NpEncoder, self).default(obj)

    def defaultify(d, depth = 0):
        if isinstance(d, dict):
            if depth == 0:
                return defaultdict(
                    lambda: defaultdict(
                        lambda: defaultlist(lambda: 0)
                    ),
                    {k: defaultify(v, depth + 1) for k, v in d.items()}
                )
            if depth == 1:
                return defaultdict(
                    lambda: defaultlist(lambda: 0),
                    {k: defaultify(v, depth + 1) for k, v in d.items()}
                )

        elif isinstance(d, list):
            tmp = defaultlist(lambda: 0)
            tmp.extend(d)
            return tmp
        else:
            return d


    def compute_relative_change(df,
                                ref_date=datetime.datetime(2020, 3, 1)):
        # Relative change
        # Compute total trips by origin
        trips_by_day_by_place = df.groupby(['date', 'origin_area_code', 'destination_area_code']).sum().reset_index()
        trips_by_day_by_place['weekday'] = trips_by_day_by_place['date'].apply(lambda x: x.weekday())

        # Compute the reference (median on that weekday in period before ref_data)
        tot_trips_ref = trips_by_day_by_place[trips_by_day_by_place.date < ref_date].groupby(
            ['weekday', 'origin_area_code', 'destination_area_code'])['all'].median().reset_index()
        # import pdb;pdb.set_trace()
        # Merge with actual data on weekday
        df_change = pd.merge(trips_by_day_by_place, tot_trips_ref,
                             on=['weekday', 'origin_area_code', 'destination_area_code'],
                             how='left',
                             suffixes=['', '_ref']).fillna(5)


        # Compute relative change
        # df_change['rel_change']=(df_change['all'] - df_change['all_ref'])/df_change['all_ref']

        return df_change


    def update_data_out(kommune, idx, data_):
        data = data_

        # import pdb;pdb.set_trace()

        # Compute flow into kommune. This counts how many from the kommune are going
        # to work in other kommunes
        data_kommune_is_target = data.loc[data.target_kommune == kommune]
        neighbors = set(data_kommune_is_target.source_kommune)
        if len(neighbors) > 0:
            neighbors = sorted(neighbors | {kommune})

        for neighbor in neighbors:  # putting `kommune` in here as a nighbor to `kommune` so if it has 0 within flow then that will show in the output data

            # Compute how many people that live in `kommune` and work in `neighbor`
            flow_from_neighbor = data_kommune_is_target.loc[data_kommune_is_target.source_kommune == neighbor].sum()
            data_out[kommune][neighbor][idx] += flow_from_neighbor.n_crisis

            # Add this flow to the total flow inside the kommune. This way, its count will
            # represent the number of people in that kommune that go to work *anywhere*
            data_out[kommune]["_" + kommune][idx] += flow_from_neighbor.n_crisis

        # Compute flow out of kommune. This counts how many people that live outside
        # of the kommune and go to work the kommune
        data_kommune_is_source = data.loc[data.source_kommune == kommune]
        neighbors = set(data_kommune_is_source.target_kommune)
        if len(neighbors) > 0:
            neighbors = sorted(neighbors | {kommune})

        for neighbor in neighbors:
            # Compute how many people that live elsewhere and work in `kommune`
            flow_to_neighbor = data_kommune_is_source.loc[data_kommune_is_source.target_kommune == neighbor].sum()
            data_out[kommune][neighbor][idx] += flow_to_neighbor.n_crisis

            # Add this flow to total outflow from kommune so it represents how many people
            # *from anywhere* that commute here during working hours
            data_out[kommune]["_" + kommune][idx] += flow_to_neighbor.n_crisis


    PATH_OUT = r"/home/petem/HOPE/WorldCovid19/covid19.compute.dtu.dk/static/data/telco_map_new.json"
    PATH_IN_ZIP = r"/data/ctdk/notebooks/"
    PATH_IN_DAT = r"/data/ctdk/raw/"

    # Load data
    data = pd.read_csv(PATH_IN_DAT + 'df_safe.csv.gz', parse_dates=['date'],
                      dtype={'origin_area_code': 'int', 'destination_area_code': 'int'})
    # removing zip code which does not exist in zip codes. Assuming it's Christiansø.
    data = data[(data.origin_area_code != 411) & (data.destination_area_code != 411)]
    #df2 = pd.read_csv(PATH_IN_DAT + 'df_safe_within.csv.gz', parse_dates=['date'],
    #                  dtype={'origin_area_code': 'int', 'destination_area_code': 'int'})

    data = compute_relative_change(data)  # .set_index('date')
    #df2 = compute_relative_change(df2)  # .set_index('date')
    #data = pd.concat([df1, df2])
    #date_min = max(df1.date.min(),df2.date.min())
    #date_max = min(df1.date.max(),df2.date.max())

    zips = pd.read_csv(PATH_IN_ZIP + 'zipcodes.csv')
    zips = zips.drop_duplicates(subset=['city_code', 'city']).set_index('city_code')
    zips['city'] = zips['city'].replace({"Aarhus":"Århus","Vesthimmerlands":"Vesthimmerland"}).str.replace('-',' ')

    data = pd.merge(data, zips[['city']], how='left', left_on='origin_area_code', right_on='city_code').rename(
        columns={'city': 'source_kommune'})
    data = pd.merge(data, zips[['city']], how='left', left_on='destination_area_code', right_on='city_code').rename(
        columns={'city': 'target_kommune'}).set_index('date')
    data = data.rename(columns={'all_ref': 'n_baseline', 'all': 'n_crisis', 'rel_change': 'percent_change'})


    # TODO: THIS IS A TEMPORARY FIX. ZIPCODE 411 IS MISSING FROM THE ZIPS FILE
    data.dropna(axis=0,inplace=True)
    #data = data[data.index <= date_max]

    data = data.astype({'n_crisis':'int'})

    path_boo = os.path.exists(PATH_OUT)
    if path_boo:
        with open(PATH_OUT, 'r') as fp:
            data_out = json.load(fp)
            data_out = defaultify(data_out, 0)
        start = len(data_out['_meta']['datetime'])
    else:
        data_out = defaultdict(lambda: defaultdict(lambda: defaultlist(lambda: 0)))

        start = 0 


        start = 0

    fn_days = data.index.unique().sort_values().strftime("%Y-%m-%d %H:%H:%H").to_list()

    for idx, fn_day in tqdm(enumerate(fn_days[start:], start), total=len(fn_days[start:])):
        data_ = data.loc[fn_day]

        # Get list of municipalities
        data_nn = data_.loc[(data_.source_kommune.notnull()) & (data_.target_kommune.notnull())]
        kommunes = sorted(set(data_nn.source_kommune.tolist() + data_nn.target_kommune.tolist()))

        # Filter list of names to remove inconsistencies.
        # kommunes = [k if k not in to_actual else to_actual[k] for k in kommunes]
        # data_pop = data_[['target_kommune', 'n_crisis', 'n_baseline']].groupby('target_kommune').sum()

        for kommune in kommunes:
            update_data_out(kommune, idx, data_)  # , data_pop)

    data_out['_meta']['defaults'] = {}
    data_out['_meta']['variables'] = {}
    data_out['_meta']['datetime'] = []

    # Time
    data_out['_meta']['datetime'] = data.index.unique().sort_values().strftime("%Y-%m-%d %H:%H:%H").to_list()

    # Get max values
    data_out['_meta']['variables']['Max'] = 0
    for source in data_out:
        if source == '_meta':
            continue
        for target, data in data_out[source].items():
            if "_" + source == target:
                data_out['_meta']['variables']['Max'] = max(
                    data_out['_meta']['variables']['Max'],
                    max(data) 
                )

    # Add to _meta
    data_out['_meta']['radioOptions'] = ['percent_change', 'crisis', 'baseline']
    data_out['_meta']['defaults']['radioOption'] = 'percent_change'
    data_out['_meta']['defaults']['t'] = len(fn_days)-1
    data_out['_meta']['defaults']['idx0or1'] = 0
    data_out['_meta']['variables']['legend_label_count'] = "Number of trips"
    data_out['_meta']['variables']['legend_label_relative'] = "Percent change"

    with open('/home/petem/HOPE/WorldCovid19/utils/data/country-bb.json') as fp:
        cbb = json.load(fp)['DK'][1]

    data_out['_meta']['defaults']['latMin'] = cbb[1]
    data_out['_meta']['defaults']['latMax'] = cbb[3]
    data_out['_meta']['defaults']['lonMin'] = cbb[0]
    data_out['_meta']['defaults']['lonMax'] = cbb[2]

    with open(PATH_OUT,'w') as f:

        json.dump(data_out,f, default = convert)
        #json.dump(data_out, f, cls=NpEncoder)


if __name__ == "__main__":
    os.chdir("../../")
    run('Denmark')












